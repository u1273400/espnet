++ train_set=train_nodev
++ train_dev=train_dev
++ lm_test=test
++ recog_set='train_dev test'
++ '[' -1 -le -1 ']'
++ '[' 100 -ge -1 ']'
++ echo 'stage -1: Data Download'
stage -1: Data Download
++ mkdir -p ./downloads
++ local/download_and_untar.sh ./downloads http://www.speech.cs.cmu.edu/databases/an4/
./downloads http://www.speech.cs.cmu.edu/databases/an4/
/home/john/src/python/espnet/egs/an4/asr1
local/download_and_untar.sh: an4 directory already exists in ./downloads
skipping..++ '[' -1 -le 0 ']'
++ '[' 100 -ge 0 ']'
++ echo 'stage 0: Data preparation'
stage 0: Data preparation
++ mkdir -p data/train data/test exp
++ read -p 'press enter to continue..'
press enter to continue..++ '[' '!' -f ./downloads/an4/README ']'
++ python local/data_prep.py ./downloads/an4 /home/john/src/python/espnet/egs/an4/asr1/../../../tools/kaldi/tools/sph2pipe_v2.5/sph2pipe
++ for x in test train
++ for f in text wav.scp utt2spk
++ sort data/test/text -o data/test/text
++ for f in text wav.scp utt2spk
++ sort data/test/wav.scp -o data/test/wav.scp
++ for f in text wav.scp utt2spk
++ sort data/test/utt2spk -o data/test/utt2spk
++ utils/utt2spk_to_spk2utt.pl data/test/utt2spk
++ for x in test train
++ for f in text wav.scp utt2spk
++ sort data/train/text -o data/train/text
++ for f in text wav.scp utt2spk
++ sort data/train/wav.scp -o data/train/wav.scp
++ for f in text wav.scp utt2spk
++ sort data/train/utt2spk -o data/train/utt2spk
++ utils/utt2spk_to_spk2utt.pl data/train/utt2spk
++ feat_tr_dir=dump/train_nodev/deltafalse
++ mkdir -p dump/train_nodev/deltafalse
++ feat_dt_dir=dump/train_dev/deltafalse
++ mkdir -p dump/train_dev/deltafalse
++ '[' -1 -le 1 ']'
++ '[' 100 -ge 1 ']'
++ echo 'stage 1: Feature Generation'
stage 1: Feature Generation
++ read -p 'press enter to continue..'
press enter to continue..++ fbankdir=fbank
++ for x in test train
++ steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/test exp/make_fbank/test fbank
steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/test exp/make_fbank/test fbank
steps/make_fbank_pitch.sh: moving data/test/feats.scp to data/test/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/test
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for test
++ utils/fix_data_dir.sh data/test
fix_data_dir.sh: kept all 130 utterances.
fix_data_dir.sh: old files are kept in data/test/.backup
++ for x in test train
++ steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/train exp/make_fbank/train fbank
steps/make_fbank_pitch.sh --cmd run.pl --nj 8 --write_utt2num_frames true data/train exp/make_fbank/train fbank
steps/make_fbank_pitch.sh: moving data/train/feats.scp to data/train/.backup
utils/validate_data_dir.sh: Successfully validated data-directory data/train
steps/make_fbank_pitch.sh: [info]: no segments file exists: assuming wav.scp indexed by utterance.
steps/make_fbank_pitch.sh: Succeeded creating filterbank and pitch features for train
++ utils/fix_data_dir.sh data/train
fix_data_dir.sh: kept all 948 utterances.
fix_data_dir.sh: old files are kept in data/train/.backup
++ utils/subset_data_dir.sh --first data/train 100 data/train_dev
utils/subset_data_dir.sh: reducing #utt from 948 to 100
+++ wc -l
++ n=848
++ utils/subset_data_dir.sh --last data/train 848 data/train_nodev
utils/subset_data_dir.sh: reducing #utt from 948 to 848
++ compute-cmvn-stats scp:data/train_nodev/feats.scp data/train_nodev/cmvn.ark
compute-cmvn-stats scp:data/train_nodev/feats.scp data/train_nodev/cmvn.ark 
LOG (compute-cmvn-stats[5.5.459~1453-93bdc]:main():compute-cmvn-stats.cc:168) Wrote global CMVN stats to data/train_nodev/cmvn.ark
LOG (compute-cmvn-stats[5.5.459~1453-93bdc]:main():compute-cmvn-stats.cc:171) Done accumulating CMVN stats for 848 utterances; 0 had errors.
++ dump.sh --cmd run.pl --nj 8 --do_delta false data/train_nodev/feats.scp data/train_nodev/cmvn.ark exp/dump_feats/train dump/train_nodev/deltafalse
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/dump.sh --cmd run.pl --nj 8 --do_delta false data/train_nodev/feats.scp data/train_nodev/cmvn.ark exp/dump_feats/train dump/train_nodev/deltafalse
++ dump.sh --cmd run.pl --nj 8 --do_delta false data/train_dev/feats.scp data/train_nodev/cmvn.ark exp/dump_feats/dev dump/train_dev/deltafalse
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/dump.sh --cmd run.pl --nj 8 --do_delta false data/train_dev/feats.scp data/train_nodev/cmvn.ark exp/dump_feats/dev dump/train_dev/deltafalse
++ for rtask in '${recog_set}'
++ feat_recog_dir=dump/train_dev/deltafalse
++ mkdir -p dump/train_dev/deltafalse
++ dump.sh --cmd run.pl --nj 8 --do_delta false data/train_dev/feats.scp data/train_nodev/cmvn.ark exp/dump_feats/recog/train_dev dump/train_dev/deltafalse
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/dump.sh --cmd run.pl --nj 8 --do_delta false data/train_dev/feats.scp data/train_nodev/cmvn.ark exp/dump_feats/recog/train_dev dump/train_dev/deltafalse
++ for rtask in '${recog_set}'
++ feat_recog_dir=dump/test/deltafalse
++ mkdir -p dump/test/deltafalse
++ dump.sh --cmd run.pl --nj 8 --do_delta false data/test/feats.scp data/train_nodev/cmvn.ark exp/dump_feats/recog/test dump/test/deltafalse
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/dump.sh --cmd run.pl --nj 8 --do_delta false data/test/feats.scp data/train_nodev/cmvn.ark exp/dump_feats/recog/test dump/test/deltafalse
++ dict=data/lang_1char/train_nodev_units.txt
++ echo 'dictionary: data/lang_1char/train_nodev_units.txt'
dictionary: data/lang_1char/train_nodev_units.txt
++ '[' -1 -le 2 ']'
++ '[' 100 -ge 2 ']'
++ echo 'stage 2: Dictionary and Json Data Preparation'
stage 2: Dictionary and Json Data Preparation
++ mkdir -p data/lang_1char/
++ echo '<unk> 1'
++ cut -f 2- '-d '
++ awk '{print $0 " " NR+1}'
++ grep -v -e '^\s*$'
++ uniq
++ sort
++ tr ' ' '\n'
++ text2token.py -s 1 -n 1 data/train_nodev/text
++ wc -l data/lang_1char/train_nodev_units.txt
28 data/lang_1char/train_nodev_units.txt
++ data2json.sh --feat dump/train_nodev/deltafalse/feats.scp data/train_nodev data/lang_1char/train_nodev_units.txt
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/data2json.sh --feat dump/train_nodev/deltafalse/feats.scp data/train_nodev data/lang_1char/train_nodev_units.txt
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/train_nodev/deltafalse/feats.scp data/train_nodev/tmp-adD9d/input_1/shape.scp
++ data2json.sh --feat dump/train_dev/deltafalse/feats.scp data/train_dev data/lang_1char/train_nodev_units.txt
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/data2json.sh --feat dump/train_dev/deltafalse/feats.scp data/train_dev data/lang_1char/train_nodev_units.txt
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/train_dev/deltafalse/feats.scp data/train_dev/tmp-HHSMq/input_1/shape.scp
++ for rtask in '${recog_set}'
++ feat_recog_dir=dump/train_dev/deltafalse
++ data2json.sh --feat dump/train_dev/deltafalse/feats.scp data/train_dev data/lang_1char/train_nodev_units.txt
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/data2json.sh --feat dump/train_dev/deltafalse/feats.scp data/train_dev data/lang_1char/train_nodev_units.txt
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/train_dev/deltafalse/feats.scp data/train_dev/tmp-ocVND/input_1/shape.scp
++ for rtask in '${recog_set}'
++ feat_recog_dir=dump/test/deltafalse
++ data2json.sh --feat dump/test/deltafalse/feats.scp data/test data/lang_1char/train_nodev_units.txt
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/data2json.sh --feat dump/test/deltafalse/feats.scp data/test data/lang_1char/train_nodev_units.txt
/home/john/src/python/espnet/egs/an4/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/test/deltafalse/feats.scp data/test/tmp-qeftt/input_1/shape.scp
++ '[' -z ']'
+++ basename conf/lm
++ lmtag=lm
++ '[' true = true ']'
++ lmtag=lm_word100
++ lmexpname=train_rnnlm_pytorch_lm_word100
++ lmexpdir=exp/train_rnnlm_pytorch_lm_word100
++ mkdir -p exp/train_rnnlm_pytorch_lm_word100
++ '[' -1 -le 3 ']'
++ '[' 100 -ge 3 ']'
++ echo 'stage 3: LM Preparation'
stage 3: LM Preparation
++ '[' true = true ']'
++ lmdatadir=data/local/wordlm_train
++ lmdict=data/local/wordlm_train/wordlist_100.txt
++ mkdir -p data/local/wordlm_train
++ cut -f 2- '-d ' data/train_nodev/text
++ cut -f 2- '-d ' data/train_dev/text
++ cut -f 2- '-d ' data/test/text
++ text2vocabulary.py -s 100 -o data/local/wordlm_train/wordlist_100.txt data/local/wordlm_train/train.txt
WARNING:root:OOV rate = 0.00 %
++ run.pl --gpu 1 exp/train_rnnlm_pytorch_lm_word100/train.log lm_train.py --config conf/lm.yaml --ngpu 1 --backend pytorch --verbose 1 --outdir exp/train_rnnlm_pytorch_lm_word100 --tensorboard-dir tensorboard/train_rnnlm_pytorch_lm_word100 --train-label data/local/wordlm_train/train.txt --valid-label data/local/wordlm_train/valid.txt --test-label data/local/wordlm_train/test.txt --resume --dict data/local/wordlm_train/wordlist_100.txt
++ '[' -z ']'
+++ basename conf/train_mtlalpha1.0
++ expname=train_nodev_pytorch_train_mtlalpha1.0
++ false
++ expdir=exp/train_nodev_pytorch_train_mtlalpha1.0
++ mkdir -p exp/train_nodev_pytorch_train_mtlalpha1.0
++ '[' -1 -le 4 ']'
++ '[' 100 -ge 4 ']'
++ echo 'stage 4: Network Training'
stage 4: Network Training
++ run.pl --gpu 1 exp/train_nodev_pytorch_train_mtlalpha1.0/train.log asr_train.py --config conf/train_mtlalpha1.0.yaml --ngpu 1 --backend pytorch --outdir exp/train_nodev_pytorch_train_mtlalpha1.0/results --tensorboard-dir tensorboard/train_nodev_pytorch_train_mtlalpha1.0 --debugmode 1 --dict data/lang_1char/train_nodev_units.txt --debugdir exp/train_nodev_pytorch_train_mtlalpha1.0 --minibatches 0 --verbose 1 --resume --train-json dump/train_nodev/deltafalse/data.json --valid-json dump/train_dev/deltafalse/data.json
++ '[' -1 -le 5 ']'
++ '[' 100 -ge 5 ']'
++ echo 'stage 5: Decoding'
stage 5: Decoding
++ nj=8
++ pids=()
++ for rtask in '${recog_set}'
++ pids+=($!)
++ for rtask in '${recog_set}'
++ pids+=($!)
++ i=0
++ for pid in '"${pids[@]}"'
++ wait 20832
+++ basename conf/decode_ctcweight1.0
+++ basename conf/decode_ctcweight1.0
++ decode_dir=decode_test_decode_ctcweight1.0_lm_word100
++ decode_dir=decode_train_dev_decode_ctcweight1.0_lm_word100
++ '[' true = true ']'
++ '[' true = true ']'
++ recog_opts='--word-rnnlm exp/train_rnnlm_pytorch_lm_word100/rnnlm.model.best'
++ recog_opts='--word-rnnlm exp/train_rnnlm_pytorch_lm_word100/rnnlm.model.best'
++ feat_recog_dir=dump/test/deltafalse
++ feat_recog_dir=dump/train_dev/deltafalse
++ splitjson.py --parts 8 dump/test/deltafalse/data.json
++ splitjson.py --parts 8 dump/train_dev/deltafalse/data.json
2020-06-29 09:45:41,486 (splitjson:43) INFO: /home/john/anaconda3/bin/python3 /home/john/src/python/espnet/egs/an4/asr1/../../../utils/splitjson.py --parts 8 dump/train_dev/deltafalse/data.json
2020-06-29 09:45:41,486 (splitjson:43) INFO: /home/john/anaconda3/bin/python3 /home/john/src/python/espnet/egs/an4/asr1/../../../utils/splitjson.py --parts 8 dump/test/deltafalse/data.json
2020-06-29 09:45:41,490 (splitjson:55) INFO: number of utterances = 100
++ ngpu=0
++ run.pl JOB=1:8 exp/train_nodev_pytorch_train_mtlalpha1.0/decode_train_dev_decode_ctcweight1.0_lm_word100/log/decode.JOB.log asr_recog.py --config conf/decode_ctcweight1.0.yaml --ngpu 0 --backend pytorch --debugmode 1 --verbose 1 --recog-json dump/train_dev/deltafalse/split8utt/data.JOB.json --result-label exp/train_nodev_pytorch_train_mtlalpha1.0/decode_train_dev_decode_ctcweight1.0_lm_word100/data.JOB.json --model exp/train_nodev_pytorch_train_mtlalpha1.0/results/model.loss.best --word-rnnlm exp/train_rnnlm_pytorch_lm_word100/rnnlm.model.best
2020-06-29 09:45:41,577 (splitjson:55) INFO: number of utterances = 130
++ ngpu=0
++ run.pl JOB=1:8 exp/train_nodev_pytorch_train_mtlalpha1.0/decode_test_decode_ctcweight1.0_lm_word100/log/decode.JOB.log asr_recog.py --config conf/decode_ctcweight1.0.yaml --ngpu 0 --backend pytorch --debugmode 1 --verbose 1 --recog-json dump/test/deltafalse/split8utt/data.JOB.json --result-label exp/train_nodev_pytorch_train_mtlalpha1.0/decode_test_decode_ctcweight1.0_lm_word100/data.JOB.json --model exp/train_nodev_pytorch_train_mtlalpha1.0/results/model.loss.best --word-rnnlm exp/train_rnnlm_pytorch_lm_word100/rnnlm.model.best
run.pl: 8 / 8 failed, log is in exp/train_nodev_pytorch_train_mtlalpha1.0/decode_train_dev_decode_ctcweight1.0_lm_word100/log/decode.*.log
++ (( ++i ))
++ for pid in '"${pids[@]}"'
++ wait 20833
run.pl: 8 / 8 failed, log is in exp/train_nodev_pytorch_train_mtlalpha1.0/decode_test_decode_ctcweight1.0_lm_word100/log/decode.*.log
++ (( ++i ))
++ '[' 2 -gt 0 ']'
++ echo 'bash: 2 background jobs are failed.'
bash: 2 background jobs are failed.
++ false
